<!DOCTYPE html>
<html lang="en">
<head>
  <title>Peer Evaluation Form</title>
  <link rel="icon" type="image/x-icon" href="../images/favicon.png">
   	<link rel="stylesheet" href="../styles/minimal-table.css" type="text/css">
</head>
<body>
    <!-- Site navigation menu -->
    <ul class="navbar">
      <li><a href="../index.html">Home</a>
      <li><a href="process.html">Process Support</a>
    </ul>
    
    <h1>Azel's Ethical Evaluation</h1><br>
    <p>Team name: <u>5012</u></p>

    <p>Individual Ethical Evaluation</p>
    <P>- Ethical Aspects of the topic</P>
    <p>- Ethical issues relating to the technology</p>
    <p>- Your personal reflections</p>
    <p>- Your recommendations</p>
    <p>- Significance of opportunities, risks and choices posed by Autonomous Vehicles</p>
    <p>- In depth elboration of the computer ethics (Asimov's Three Laws)</p>
    A Three-step Strategy for Approaching Computer Ethics Issues (*)
•Step 1: Identify a practice involving information and communications technology, or a feature of 
that technology, that is controversial from a moral perspective. 
•1a. Disclose any hidden (or opaque) features or issues that have moral implications 
•1b. If the ethical issue is descriptive, assess the sociological implications for relevant social institutions and socio-
demographic and populations. 
•1c. If the ethical issue is also normative, determine whether there are any specific guidelines, that is, 
professional codes that can help you resolve the issue. 
•Step 2. Analyze the ethical issue by clarifying concepts and situating it in a context. 
•2a. If a policy vacuums exists, go to Step 2b; otherwise, go to Step 3. 
•2b. Clear up any conceptual muddles involving the policy vacuum and go to Step 3.
•Step 3. Deliberate on the ethical issue. The deliberation process requires two stages. 
•3a. Apply one or more ethical theories (see Chapter 2) to the analysis of the moral issue, and then go to Step 3b. 
•3b. Justify the position you reached by evaluating it via the standards and criteria for successful logic 
argumentation (see Chapter 3).

1. A robot may not injure a human being or, 
through inaction, allow a human being to 
come to harm.
Asimovs Three Laws
The Three Laws, quoted as being from the "Handbook of Robotics, 56th 
Edition, 2058 A.D. introduced in Asimov’s 1942 short story "Runaround" 
in  the March 1942 issue of Astounding Science Fiction.
2. A robot must obey the orders given to 
it by human beings except where such 
orders would conflict with the First Law.
3. A robot must protect its own 
existence as long as such protection 
does not conflict with the First or 
Second Laws.
<p>When the topic of Autonomous Vehicles is addressed the idea of ethics- the standard of what is believed to be right and wrong is a huge factor that needs to be taken into consideration.</p>
<p>Some ethical issues we face with AV's are Cyber Hacking, and user privacy/data collection being at risk. As most Autonomous Vehicles require third party software and hardware components in order to manufacture the cars, the vulnerability for Cyberhackers to perpetrate software systems and take control of these systems is high. Thankfully, there are Professional Codes of Conduct to help combat some of these issues users may face. The IEEE- CS SE Code of Ethics helps ensure the Software Engineers are consistently acting in the best interest of the public and that they always promote and ethical approach to their profession (Gotterbarn, D., Miller, K., & Rogerson, S. (1999).</p>

<p>References in Azel's Individual Ethical Evaluation:</p>
<li>Gotterbarn, D., Miller, K., & Rogerson, S. (1999). Computer society and ACM 
  approve software engineering code of ethics. Computer, 32(10), 84-88</li>